Technology companies will have to take more action to keep children safe on the internet, following the introduction of the Online Safety Act. 
But the new rules will not come in until 2025 - and critics say they do not go far enough.
Children aged eight to 17 spend between two and five hours online per day, research by the communications regulator Ofcom suggests. Time spent online increases with age.
Nearly every child over 12 has a mobile phone and almost all of them watch videos on platforms such as YouTube or TikTok.
Four in five teenagers who go online say they have used artificial intelligence (AI) tools such as ChatGPT or Snapchat's MyAI.
About half of children over 12 think being online is good for their mental health, according to Ofcom.
But there is a significant minority for whom that is not the case. One in eight children aged eight to 17 said someone had been nasty or hurtful to them on social media, or messaging apps.
The Children's Commissioner said that half of the 13-year-olds her team surveyed reported seeing "hardcore, misogynistic" pornographic material on social media sites.
Two-thirds of parents say they use controls to limit what their children see online, according to Internet Matters, a safety organisation set up by some of the big UK-based internet companies.
It has a list of parental controls available and step-by-step guides on how to use them.
For example, parents who want to reduce the likelihood of their children seeing unsuitable material on YouTube - the most popular platform for young people in the UK - can set up the "kids" version, which filters out adult content. 
For older children using the main site, parents can set up supervised accounts, which let them review the sites their children visit.
Supervision can also be set up on Facebook messenger, via its Family Centre.
TikTok says its family pairing tool lets parents decide whether to make a teenager's account private.
Instagram's parental controls include daily time limits, scheduled break times and can list accounts their children have reported. 
But these controls are not fool-proof. Ofcom data suggests about one in 20 children uses workarounds.
Phone networks may block some explicit websites until a user has demonstrated they are over 18. 
Some also have parental controls that can limit the websites children can visit on their phones.
Android and Apple phones and tablets have apps and systems parents can use.
These can block or limit access to specific apps, restrict explicit content, prevent purchases and monitor browsing.
Apple has Screen Time and Google has Family Link. There are similar apps available from third-party developers.
Broadband services also have parental controls to filter certain types of content.  
Game console controls also let parents ensure age-appropriate gaming and control in-game purchases.
Talking to children about online safety and being interested in what they do online is also important, according to the NSPCC.
It recommends making discussions about it part of daily conversation, just like a chat about their day at school, which can make it easier for children to share any concerns they have. 
The government says the Online Safety Act - due to come into force in the second half of 2025 - puts the onus on social media firms and search engines to protect children from some legal-but-harmful material. 
Platforms will also have to show they are committed to removing illegal content, including:
Pornography sites will have to stop children viewing content, by checking ages.
Other new offences have been created, including:
The act also makes it easier for bereaved parents to obtain information about their children from technology companies.
The regulator Ofcom has been given extra enforcement powers to ensure companies comply with the new rules and has published draft codes for them to follow. 
It says the companies must reconfigure the algorithms deciding which content users see, to ensure the most harmful material does not appear in children's feeds and reduce the visibility and prominence of other damaging content. 
Chief executive Dame Melanie Dawes warned any company failing to follow the rules could have their minimum user age raised to 18. 
And Technology Secretary Michelle Donelan urged big tech to take the codes seriously: 
"Engage with us and prepare," she said. 
"Do not wait for enforcement and hefty fines - step up to meet your responsibilities and act now."
Some parents of children who died after exposure to harmful online content have called the new rules "insufficient" and criticised the delay before they come into force. 
Ian Russell, father of Mollie, and Esther Ghey, mother of Brianna, are part of a group of bereaved parents that signed an open letter to Prime Minister Rishi Mr Sunak and Leader of the Opposition Sir Keir Starmer, calling for more action. 
They want a commitment to strengthen the Online Safety Act in the first half of the next Parliament and mental health and suicide prevention added to the school curriculum.
"While we will study Ofcom's latest proposals carefully, we have so far been disappointed by their lack of ambition," they add in the letter.
Meta and Snapchat said they already had extra protections for under-18s and highlighted their existing parental tools.
"As a platform popular with young people, we know we have additional responsibilities to create a safe and positive experience," a Snapchat representative said.
A Meta representative said it wanted young people "to connect with others in an environment where they feel safe".
"Content that incites violence, encourages suicide, self-injury or eating disorders breaks our rules - and we remove that content when we find it," they said.
A number of other technology companies contacted by BBC News declined to respond to the draft measures.
