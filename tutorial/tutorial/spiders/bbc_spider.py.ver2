import scrapy
from scrapy.http import HtmlResponse
from scrapy import signals
import re
import os

class BBCSpider(scrapy.Spider):
    name = "bbc"
    start_urls = [
            "https://www.bbc.com/",
    ]

    access_url_set = set()

    def write_article(self, url, tittle, content_blocks):
        directory = "web_data/" + url
        if not os.path.exists(directory):
            #print("Hardy create: ", directory)
            os.makedirs(directory)
        
        # tittle
        tittle_path = os.path.join(directory, "tittle")
        with open(tittle_path, 'w', encoding='utf-8') as tittle_f:
            tittle_f.write(tittle)
        # content
        content_path = os.path.join(directory, "content")
        with open(content_path, 'w', encoding='utf-8') as content_f:
            for block in content_blocks:
                content_f.write(block + '\n')

    def parse_article(self, response):
        url = response.request.url
        if "/sport" in url or "/weather" in url:
            tittle = response.xpath('//*[@id="main-heading"]/span/text()').get()
            content_blocks = response.xpath('//*[@id="main-content"]/article/div[@data-component="text-block"]//p//text()').getall()
        else:
            tittle = response.xpath('//*[@id="main-content"]/article/div[@data-component="headline-block"]/h1/text()').get()
            content_blocks = response.xpath('//*[@id="main-content"]/article/div[@data-component="text-block"]/p/text()').getall()

        self.write_article(url, tittle, content_blocks)

    def parse(self, response):
        url = response.request.url

        self.access_url_set.add(url)

        if "/articles/" in url:
            self.parse_article(response)
        else:
            relative_links = response.xpath('//a/@href').getall()
            absolute_links = [response.urljoin(link) for link in relative_links]

            for link in absolute_links:
                if not link.startswith("http"):
                    continue
                # out of site
                if not link.startswith("https://www.bbc.com/"):
                    continue

                # 跳过带锚点的 URL
                if "#" in link:
                    continue

                if link in self.access_url_set:
                    continue
                self.access_url_set.add(link)

                print("hardy link: ", link)
                yield response.follow(link, callback = self.parse)
